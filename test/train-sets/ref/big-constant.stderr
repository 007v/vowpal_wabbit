Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
creating cache_file = train-sets/big-constant.dat.cache
Reading datafile = train-sets/big-constant.dat
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
0.102639 0.102639            1            1.0 1000.3204 1000.0000       23
0.357116 0.611592            2            2.0 999.5383 1000.3204       23
3.681182 7.005248            4            4.0 1001.4026 998.1745       23
4.045717 4.410252            8            8.0 1000.9437 999.5552       23
3.394786 2.743855           16           16.0 1003.0878 1000.4109       23
2.561833 1.728880           32           32.0 1000.0400 999.6091       23
1.986501 1.411169           64           64.0 1001.1794 1000.0136       23
1.758407 1.530312          128          128.0 998.8786 999.7438       23
1.331073 0.903739          256          256.0 1000.6227 1000.4048       23
0.934660 0.538248          512          512.0 1001.3120 1000.6467       23
0.586746 0.238832         1024         1024.0 1001.0090 1000.9319       23
0.332557 0.078367         2048         2048.0 999.2150 999.4192       23
0.174530 0.016504         4096         4096.0 999.5161 999.5197       23
0.088185 0.001839         8192         8192.0 999.5844 999.5401       23

finished run
number of examples per pass = 100
passes used = 100
weighted example sum = 10000.000000
weighted label sum = 9998961.669922
average loss = 0.072273
best constant = 999.896179
total feature number = 230000
