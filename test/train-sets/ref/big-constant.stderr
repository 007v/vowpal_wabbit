Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
creating cache_file = train-sets/big-constant.dat.cache
Reading datafile = train-sets/big-constant.dat
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
0.000000 0.000000            1            1.0 1000.3204 1000.3204       23
0.106570 0.213139            2            2.0 999.5383 1000.0000       23
3.099322 6.092074            4            4.0 1001.4026 998.1102       23
3.821615 4.543908            8            8.0 1000.9437 999.7509       23
3.259716 2.697818           16           16.0 1003.0878 1000.3894       23
2.513634 1.767552           32           32.0 1000.0400 999.6415       23
1.969306 1.424978           64           64.0 1001.1794 999.9897       23
1.760972 1.552638          128          128.0 998.8786 999.7580       23
1.338749 0.916527          256          256.0 1000.6227 1000.3911       23
0.943278 0.547806          512          512.0 1001.3120 1000.6463       23
0.593569 0.243860         1024         1024.0 1001.0090 1000.9182       23
0.336978 0.080387         2048         2048.0 999.2150 999.4276       23
0.176989 0.017001         4096         4096.0 999.5161 999.5184       23
0.089446 0.001903         8192         8192.0 999.5844 999.5394       23

finished run
number of examples per pass = 100
passes used = 100
weighted example sum = 10000.000000
weighted label sum = 9998961.669922
average loss = 0.073307
best constant = 999.896179
total feature number = 230000
