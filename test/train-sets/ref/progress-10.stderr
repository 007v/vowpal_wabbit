Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = train-sets/0001.dat
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
0.175870 0.175870           10           10.0   1.0000   0.1549       34
0.194273 0.212676           20           20.0   0.0000   0.1978      104
0.216658 0.261428           30           30.0   0.0000   0.3502       82
0.210422 0.191714           40           40.0   1.0000   0.6030       42
0.205949 0.188055           50           50.0   0.0000   0.1864       60
0.218390 0.280596           60           60.0   0.0000   0.3633      147
0.217880 0.214824           70           70.0   1.0000   0.4550      134
0.216168 0.204182           80           80.0   0.0000   0.2306      136
0.208926 0.150986           90           90.0   0.0000   0.2777      139
0.208600 0.205669          100          100.0   1.0000   0.3082       56
0.210653 0.231178          110          110.0   1.0000   0.5463       97
0.219768 0.320037          120          120.0   0.0000   0.4109      120
0.216466 0.176836          130          130.0   1.0000   0.3852       54
0.211787 0.150960          140          140.0   0.0000   0.3311       82
0.206666 0.134979          150          150.0   1.0000   0.4038      148
0.206135 0.198174          160          160.0   0.0000   0.6028       63
0.201043 0.119560          170          170.0   1.0000   0.7031       69
0.196626 0.121544          180          180.0   1.0000   0.8089       42
0.193741 0.141816          190          190.0   1.0000   0.7799       34
0.191325 0.145416          200          200.0   1.0000   0.5324       56

finished run
number of examples per pass = 200
passes used = 1
weighted example sum = 200.000000
weighted label sum = 91.000000
average loss = 0.191325
best constant = 0.455000
best constant's loss = 0.247975
total feature number = 15482
