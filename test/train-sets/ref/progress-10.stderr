Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = train-sets/0001.dat
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
0.251937 0.251937           10           10.0   1.0000   0.2252       34
0.235273 0.218609           20           20.0   0.0000   0.2122      104
0.240933 0.252254           30           30.0   0.0000   0.3390       82
0.229268 0.194272           40           40.0   1.0000   0.5487       42
0.225421 0.210033           50           50.0   0.0000   0.2070       60
0.232323 0.266834           60           60.0   0.0000   0.3381      147
0.229978 0.215904           70           70.0   1.0000   0.4732      134
0.226350 0.200953           80           80.0   0.0000   0.2216      136
0.217414 0.145927           90           90.0   0.0000   0.2603      139
0.216708 0.210353          100          100.0   1.0000   0.3172       56
0.218363 0.234916          110          110.0   1.0000   0.5796       97
0.226828 0.319944          120          120.0   0.0000   0.4137      120
0.223097 0.178325          130          130.0   1.0000   0.4104       54
0.218314 0.156141          140          140.0   0.0000   0.3487       82
0.212769 0.135140          150          150.0   1.0000   0.4022      148
0.211703 0.195713          160          160.0   0.0000   0.6070       63
0.206611 0.125130          170          170.0   1.0000   0.7092       69
0.202222 0.127613          180          180.0   1.0000   0.7582       42
0.198761 0.136459          190          190.0   1.0000   0.7647       34
0.195768 0.138901          200          200.0   1.0000   0.5242       56

finished run
number of examples per pass = 200
passes used = 1
weighted example sum = 200.000000
weighted label sum = 91.000000
average loss = 0.195768
best constant = 0.455000
best constant's loss = 0.247975
total feature number = 15482
