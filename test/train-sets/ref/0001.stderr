Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = models/0001.model
Num weight bits = 18
learning rate = 2.56e+06
initial_t = 128000
power_t = 1
decay_learning_rate = 1
creating cache_file = train-sets/0001.dat.cache
Reading datafile = train-sets/0001.dat
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
0.000000 0.000000            1            1.0   1.0000   1.0000      290
0.000000 0.000000            2            2.0   0.0000   0.0000      608
0.000000 0.000000            4            4.0   0.0000   0.0000      794
0.125090 0.250180            8            8.0   0.0000   0.0164      860
0.241826 0.358561           16           16.0   1.0000   0.0421      128
0.288724 0.335623           32           32.0   0.0000   0.0576      176
0.301226 0.313728           64           64.0   0.0000   0.1343      350
0.299938 0.298650          128          128.0   1.0000   0.2991      620
0.241056 0.182174          256          256.0   0.0000   0.2623      410
0.121875 0.002694          512          512.0   0.0000   0.0079      278
0.060938 0.000001         1024         1024.0   1.0000   1.0000      170

finished run
number of examples per pass = 200
passes used = 8
weighted example sum = 1600.000000
weighted label sum = 728.000000
average loss = 0.039000
best constant = 0.455000
best constant's loss = 0.247975
total feature number = 717536
