HOWTO write new tests for the big_tests test suite
==================================================

There are 3 steps in writing a new test.
1. Add some variable settings to the file testDetails.mk.
2. Add code to dataSets.mk for obtaining and preparing relevant data, if it's not already there for other tests.
3. Create the "expected" files.  Note that your new tests WILL NOT EVEN RUN unless at least the 'out' and 'err' expected files exist in the right place.
Read on for details.

Step 1: testDetails.mk
----------------------

Be sure to use := and not = to set these variables.  All variables are optional.  For most tests, you will want to set the variables *.params and *.inData, but it's possible to have tests without them.  Here's a simple example of a complete test specification:

myTest.inData := $(dataDir)/ftrl-proximal/ftrl-proximal-train.prep
myTest.params := -k -d $(myTest.inData) -f 0001_ftrl.model --passes 10 --ftrl --ftrl_alpha 0.01 --ftrl_beta 0 --l1 2 --cache --holdout_off

The file that *.inData refers to is always treated as a prerequisite for running a test.  So if it's not there, an attempt will be made to obtain and/or prepare it.

By default, STDOUT will go to the file out and STDERR will go to the file err. Other created files that should be compared to "expected" files should be captured by the *.otherOutputs variable.  For example:

myTest.otherOutputs := 0001_ftrl.model

To use a non-standard executable (e.g. library_example), give its path to the *.exec variable, relative to the directory of the top level Makefile.  For example:

myTest.exec := $(TOP_MK_DIR)/../vowpalwabbit/library_example

[Not working yet] A test can have any set of targets as prerequisites, in addition to the value of *.inData, including those that merely run other tests (*.run targets) and those that check whether another test passed (*.valid targets).  List inter-test dependencies like this:

myTest.deps:	test2.run test4.valid

Some groups of tests are defined at the top of testDetails.mk.  You can add your tests to these groups or create your own groups.


Step 2: dataSets.mk
-------------------

The data will typically be under dataSets/ .  Look at dataSets.mk for examples of how to add new datasets.

Whenever you add a new data set, don't forget to add its name to the 'allData' target in dataSets.mk .


Step 3: expected files
----------------------

Every test should create files named 'out' and 'err' (even if they are empty), and possibly others.  When you run `make $testName.compare` (which is the 2nd part of running `make $testName.valid`) these files are compared to the expected files with the same names in the directory expected/$testName/ .  The $testName.compare target WILL NOT RUN unless the expected files exist in that directory.  So, the first time you create a test, you should (a) `make $testName.run`, (b) create the directory expected/$testName/, and (c) copy the expected files to that directory.  Only then will you be able to `make $testName.valid`.  If you want to share you test with others, don't forget to `git add expected/$testName/`.
